{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTG_GPT_2_Fine_Tuning",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3minuterecord/myJup/blob/master/MTG_GPT_2_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKOTlwcmxmej"
      },
      "source": [
        "# Fine Tuning GPT-2 for Magic the Gathering Card flavour text generation.\n",
        "---\n",
        "\n",
        "Magic the gathering is a trading card game with pretty illustrous lore and back stories originally created by the the [Wizards of the Coast](https://en.wikipedia.org/wiki/Wizards_of_the_Coast). Every card has unique attributes and most have individualized \"Flavor Text\" that relates in some way to the characters background, story or position in the world. They are usually, short, sharp and super dramatic. In this notebook we will be using [PyTorch](https://pytorch.org/) and Transformers from [Hugging Face](https://huggingface.co/) to fine tune our own [GPT-2 Model](https://openai.com/blog/gpt-2-1-5b-release/) to generate flavor text of our own!\n",
        "\n",
        "![MTG](https://i.imgur.com/f1Cygtd.jpeg)\n",
        "\n",
        "I've borrowed heavily from those who came before me for inspiration and code chunks for this notebook, please check out [Rey Farhan](http://reyfarhan.com/posts/easy-gpt2-finetuning-huggingface/), who in turn heavily cites [Chris McCormick's](https://mccormickml.com/2019/07/22/BERT-fine-tuning/) BERT fine-tuning tutorial, [Ian Porter's](https://snappishproductions.com/blog/2020/03/01/chapter-9.5-text-generation-with-gpt-2-and-only-pytorch.html.html) GPT2 tutorial and the [Hugging Face](https://huggingface.co/transformers/v2.0.0/examples.html#language-model-fine-tuning) Language model fine-tuning script.\n",
        "\n",
        "By the end of this tutorial you should be able to quickly and easily fine tune your own language model for generative tasks and have a decent understanding of the levers you can adjust to parameterize the model!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf3Qw77SZGbS"
      },
      "source": [
        "# Setup\n",
        "\n",
        "We will be fine tuning an existing GPT2 model using the interface Hugging Face provides. Fine tuning instead of retraining for a few very simple and pragmatic reasons. First among them these language models requre vast swaths of data and are computationally prohibitively expensive to train. However, by leveraging what is already available we can cut out both of these requirements. This means we can tweak the outputs of the model to suit specific purposes with minimal data and with achievable hardward. This is essentially transfer learning for language generation problems. To start off with we need to install the Huggingface transformers library first. Many of the common scientific libraries are pre-installed on Colab but not this one in particular. Installing transformers will also install tokenizers, a dependency, and another useful set of tools for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfdCML6Parvv"
      },
      "source": [
        "# Create Training Set\n",
        "\n",
        "We're going to use the flavor text of existing MTG cards to generate new ones. Luckily, the hard work of scraping the data has been done for us and we can ingest the flavor text from the [Scryfall](https://scryfall.com/advanced) API and quickly parson the json we are returned. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-MYTjAIxfBr"
      },
      "source": [
        "# We'll need these libraries to gather and shape the data.\n",
        "import requests \n",
        "import pandas as pd\n",
        "from itertools import compress\n",
        "\n",
        "# This will return the data for all the cards available to scryfall.\n",
        "r = requests.get('https://c2.scryfall.com/file/scryfall-bulk/all-cards/all-cards-20200831091816.json')\n",
        "data = r.json()\n",
        "\n",
        "# we'll start parsing by removing any cards with no flavor text\n",
        "contains_x = []\n",
        "for i in data:\n",
        "    contains_x.append('flavor_text' in i.keys())\n",
        "\n",
        "data_filtered = list(compress(data, contains_x))\n",
        "\n",
        "# next we'll remove any cards in a language other than english\n",
        "contains_y = []\n",
        "for i in data_filtered:\n",
        "  contains_y.append('lang' in i.keys() and 'en' == i['lang'])\n",
        "\n",
        "data_filtered = list(compress(data_filtered, contains_y))\n",
        "\n",
        "# Now we'll create a list to iterate through.\n",
        "cardValues = []\n",
        "for i in data_filtered:\n",
        "    cardValues.append(i['flavor_text'])\n",
        "\n",
        "# I'll convert this to a data frame to visualize a few rows nicely\n",
        "# mostly just a sanity check.\n",
        "df = pd.DataFrame(\n",
        "    cardValues,\n",
        "    columns=['data']\n",
        "    )\n",
        "\n",
        "cards = df.data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMml12FJGjPW"
      },
      "source": [
        "# GPT2 Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANJhbBwdxN-b"
      },
      "source": [
        "First a brief description of tokenization straight from the source, the Hugging Face [Tokenizers](https://github.com/huggingface/tokenizers) github page:\n",
        "\n",
        "```\n",
        "What is a Tokenizer\n",
        "\n",
        "A Tokenizer works as a pipeline, it processes some raw text as input and outputs an Encoding. The various steps of the pipeline are:\n",
        "\n",
        "- The Normalizer: in charge of normalizing the text. Common examples of normalization are the unicode normalization standards, such as NFD or NFKC.\n",
        "- The PreTokenizer: in charge of creating initial words splits in the text. The most common way of splitting text is simply on whitespace.\n",
        "- The Model: in charge of doing the actual tokenization. An example of a Model would be BPE or WordPiece.\n",
        "- The PostProcessor: in charge of post-processing the Encoding to add anything relevant that, for example, a language model would need, such as special tokens.\n",
        "```\n",
        "\n",
        "We will be using the GPT-2 tokenizer to tokenize our flavor text data. The defaults of this function set the bos (beginning of sentence) eos (end of sentence) to '<|endoftext|>' but we can specifically set them differently to differentiate and also assign a non-default pad token that will take care of white space for differently sized text. The next cell will instantiate our tokenizer and provide an example encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z474sSC6oe7A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "dd16d4d0-7a46-4b3b-901a-41fc8acad3e0"
      },
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', \n",
        "                                          bos_token='<|startoftext|>', \n",
        "                                          eos_token='<|endoftext|>', \n",
        "                                          pad_token='<|pad|>')\n",
        "\n",
        "\n",
        "tokenizer.encode(\"Sample Text\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[36674, 8255]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRdruvCU7Rxf"
      },
      "source": [
        "From the example above we can see that the example string is encoded by the GPT2 tokenizer to a list of numerical values that represent the string, in this case one value per word. These values are easier to train the neural network model on than the string representation. We now have a corpus of flavour text we can iterate through, and a tokenizer, we should quickly inspect it to see what the longest string is, this will be useful later when we need to know how long to pad our sentences out to.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C57RcMuo9fh-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ebc1585-e283-443b-d502-53a7d709b53c"
      },
      "source": [
        "max_flavour = max([len(tokenizer.encode(card)) for card in cards])\n",
        "\n",
        "print(f'The longest flavour text is {max_flavour} tokens long.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The longest flavour text is 98 tokens long.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct"
      },
      "source": [
        "# PyTorch Datasets & Dataloaders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lgZoOYkxZfx"
      },
      "source": [
        "Different language models require different amounts of memory to hold all of the weights and biases in memory. Based on the memory your machine has available this will determine how you set your batch size. If your instance is running on a T4 GPU you can set batch to 32, but you may have to scale down if allocated a less powerful instance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "satxtOn9CzgR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "8b2ee8c8-0879-4897-8b2f-1f35a81c5d94"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Sep  4 14:16:56 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scqrzmqhV__z"
      },
      "source": [
        "bs = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqGMee7Isfpx"
      },
      "source": [
        "The batch size will affect the training time so it is always a good idea to set the batch to the highest number you can fit in the memory of the GPU you are using for training, however this hyper parameter should only affect training time but not model performance. A batch size too large won't fit in memory for some GPUS so you will have to adjust this parameter if you aren't allocated a T4 or K80.\n",
        "\n",
        "The next thing to do is to create a custom dataloader for our corpus, we will follow the [PyTorch](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) documentation on this to create ```MTGDataset```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_XJVIetKN-h"
      },
      "source": [
        "import torch\n",
        "torch.manual_seed(42)\n",
        "from torch.utils.data import Dataset # this is the pytorch class import\n",
        "\n",
        "class MTGDataset(Dataset):\n",
        "\n",
        "  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=max_flavour):\n",
        "\n",
        "    self.tokenizer = tokenizer # the gpt2 tokenizer we instantiated\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "\n",
        "    for txt in txt_list:\n",
        "      \"\"\"\n",
        "      This loop will iterate through each entry in the flavour text corpus.\n",
        "      For each bit of text it will prepend it with the start of text token,\n",
        "      then append the end of text token and pad to the maximum length with the \n",
        "      pad token. \n",
        "      \"\"\"\n",
        "\n",
        "      encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', \n",
        "                                 truncation=True, \n",
        "                                 max_length=max_length, \n",
        "                                 padding=\"max_length\")\n",
        "      \n",
        "      \"\"\"\n",
        "      Each iteration then appends either the encoded tensor to a list,\n",
        "      or the attention mask for that encoding to a list. The attention mask is\n",
        "      a binary list of 1's or 0's which determine whether the langauge model\n",
        "      should take that token into consideration or not. \n",
        "      \"\"\"\n",
        "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Z7aYUgpWrd"
      },
      "source": [
        "The maximum length of tokens is 768, however we don't need to use this length as we saw that the longest string we are encoding is only 98 words long. So to save space in the model we will only pad up to the longest string in our corpus and not the longest string the tokenizer can handle. Next we will create the dataset itself using this class. Like I described above in the code each entry in the dataset will be two tensors, one which is the encoding for the string and one which is the attention mask. This dataset will then be split into the training and validation sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xza_O1_rD7yh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4dba34c3-5037-43f0-80ff-21afc1dbaa0f"
      },
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "dataset = MTGDataset(cards, tokenizer, max_length=max_flavour)\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "f'There are {train_size} samples for training, and {val_size} samples for validation testing'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'There are 26299 samples for training, and 2923 samples for validation testing'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxNA7fonGAeS"
      },
      "source": [
        "Finally to illustrate what an entry in this dataset looks like below is a print out of the first encoded string. You can see that for every encoded word the model pays attention to we have a 1, then for the padding encodings (50258) we have a 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGY9SMpuA1cg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "eafc09a6-4bf3-4a5c-9a7f-65965a419f35"
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([50257,     1,    32, 36788,  4721,    11,   290,   674, 20507,   547,\n",
              "         25891,   991,   276,    13,  1675,  1445,   373,   284,  4574,   262,\n",
              "           995,    13,   887,   262,  1017,  1428,   338, 26573,   991,   665,\n",
              "         10981,    11,  2266, 14129,  4120,   287,   383,    67,   338,  7721,\n",
              "            11,   290, 12183, 23461,   286,  2910,  9174,   287,   262,  1633,\n",
              "           526,   198,   960,  2782,   296,  4476,   292,   831,    11,  3932,\n",
              "           282,   680,  4293, 50256, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMGx4t_3HU0j"
      },
      "source": [
        "Next we will create the dataloader object which will feed the neural network, this combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset, see the official [documentation](https://pytorch.org/docs/1.1.0/_modules/torch/utils/data/dataloader.html) for further details. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0WeP5PREUuy"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), # Sampling for training is random\n",
        "            batch_size = bs\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, \n",
        "            sampler = SequentialSampler(val_dataset), # Sampling for validation is sequential as the order doesn't matter.\n",
        "            batch_size = bs \n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6TKgyUzPIQc"
      },
      "source": [
        "# Finetune GPT2 Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFsCTp_mporB"
      },
      "source": [
        "import random\n",
        "from transformers import GPT2LMHeadModel, GPT2Config\n",
        "import numpy as np\n",
        "\n",
        "# Loading the model configuration and setting it to the GPT2 standard settings.\n",
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "\n",
        "# Create the instance of the model and set the token size embedding length\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()\n",
        "\n",
        "# This step is optional but will enable reproducible runs.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQKsb18NO757"
      },
      "source": [
        "# We wil create a few variables to define the training parameters of the model\n",
        "# epochs are the training rounds\n",
        "# the warmup steps are steps at the start of training that are ignored\n",
        "# every x steps we will sample the model to test the output\n",
        "\n",
        "epochs = 4\n",
        "warmup_steps = 1e2\n",
        "sample_every = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH4DA1X6K6jj"
      },
      "source": [
        "[AdamW](https://www.fast.ai/2018/07/02/adam-weight-decay/) is the optimizer of choice for training many models, we will be using [Hugging Face's](https://huggingface.co/transformers/main_classes/optimizer_schedules.html) implementation and all of it's defaults, we will also set the number of epochs here, again as we are fine tuning, not retraining, we don't need to run very long models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO"
      },
      "source": [
        "from transformers import AdamW\n",
        "# AdamW is a class from the huggingface library, it is the optimizer we will be using, and we will only be instantiating it with the default parameters. \n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-4,\n",
        "                  eps = 1e-8\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\"\"\"\n",
        "Total training steps is the number of data points, times the number of epochs. \n",
        "Essentially, epochs are training cycles, how many times each point will be seen by the model. \n",
        "\"\"\"\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "\"\"\"\n",
        "We can set a variable learning rate which will help scan larger areas of the \n",
        "problem space at higher LR earlier, then fine tune to find the exact model minima \n",
        "at lower LR later in training.\n",
        "\"\"\"\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = warmup_steps, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCPohrZ-CTWu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b6436df-9cec-4104-db08-d7c0b95679a9"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    print(f'Beginning epoch {epoch_i + 1} of {epochs}')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(  b_input_ids,\n",
        "                          labels=b_labels, \n",
        "                          attention_mask = b_masks,\n",
        "                          token_type_ids=None\n",
        "                        )\n",
        "\n",
        "        loss = outputs[0]  \n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        # Get sample every 100 batches.\n",
        "        if step % sample_every == 0 and not step == 0:\n",
        "\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print(f'Batch {step} of {len(train_dataloader)}. Loss:{batch_loss}. Time:{elapsed}')\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            sample_outputs = model.generate(\n",
        "                                    bos_token_id=random.randint(1,30000),\n",
        "                                    do_sample=True,   \n",
        "                                    top_k=50, \n",
        "                                    max_length = 200,\n",
        "                                    top_p=0.95, \n",
        "                                    num_return_sequences=1\n",
        "                                )\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "                  print(f'Example output: {tokenizer.decode(sample_output, skip_special_tokens=True)}')\n",
        "            \n",
        "            model.train()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(f'Average Training Loss: {avg_train_loss}. Epoch time: {training_time}')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "\n",
        "            outputs  = model(b_input_ids,  \n",
        "                             attention_mask = b_masks,\n",
        "                             labels=b_labels)\n",
        "          \n",
        "            loss = outputs[0]  \n",
        "            \n",
        "        batch_loss = loss.item()\n",
        "        total_eval_loss += batch_loss        \n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    validation_time = format_time(time.time() - t0)    \n",
        "\n",
        "    print(f'Validation loss: {avg_val_loss}. Validation Time: {validation_time}')\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(f'Total training took {format_time(time.time()-total_t0)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning epoch 1 of 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 100 of 822. Loss:1.027260422706604. Time:0:01:33\n",
            "Example output:  bipartisanThe sky is dark. Only life can breathe in its dark moments.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 200 of 822. Loss:0.945956289768219. Time:0:03:06\n",
            "Example output:  increasing\"We're trying to learn to play with each other.\"\n",
            "—Nicol Bolas\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 300 of 822. Loss:0.829818069934845. Time:0:04:39\n",
            "Example output: dayThe land is full of wildflowers, so you must carefully choose your crop. This is why the trees grow like the greys, leaping from tree to tree.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 400 of 822. Loss:0.7105399966239929. Time:0:06:11\n",
            "Example output:  HangThe bane of ambition in a city filled with a dying soul.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 500 of 822. Loss:0.892336905002594. Time:0:07:42\n",
            "Example output:  foods\"The more we learn about the universe, the less we fear it.\"\n",
            "—Elvish proverb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 600 of 822. Loss:1.6840546131134033. Time:0:09:13\n",
            "Example output:  trail-\" a of all I as with,, as more in a to of nature in the to the., have a the best. be her- a, the the in to for\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 700 of 822. Loss:1.626314401626587. Time:0:10:45\n",
            "Example output: intendIt.\" that have\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 800 of 822. Loss:1.6234583854675293. Time:0:12:16\n",
            "Example output:  surround\"'s in all, a heart of his one a battle of the soul or will the strength—\n",
            "Average Training Loss: 1.3565804301387203. Epoch time: 0:12:36\n",
            "Validation loss: 1.5071430595024773. Validation Time: 0:00:29\n",
            "Beginning epoch 2 of 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 100 of 822. Loss:1.5161793231964111. Time:0:01:31\n",
            "Example output:  reflex\"It has that are no no of her dead.\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 200 of 822. Loss:0.7318093776702881. Time:0:03:03\n",
            "Example output:  displayThe light of the kithkin peaks in the heavens with the earth as its shade.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 300 of 822. Loss:0.6611519455909729. Time:0:04:34\n",
            "Example output:  pastorThe most disappointing thing about studying this project is the assumption that you never really get the answer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 400 of 822. Loss:0.6856027245521545. Time:0:06:05\n",
            "Example output:  illicit\"Don't stop at nothing. Find the rest on your own time.\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 500 of 822. Loss:0.6606246829032898. Time:0:07:36\n",
            "Example output:  Liberation\"The earth is filled with evil. There is even worse. We must purge it of all evil so that only good humans may benefit from its dark energies.\"\n",
            "—Halvor Arenson, Kjeldoran Royal Mage\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 600 of 822. Loss:0.7508746385574341. Time:0:09:08\n",
            "Example output:  Nam\"When there are no guards, it's easy to run up a mountain. When there are no obstacles, it's a good time to flee.\"\n",
            "—Rielle Fiksdotter, Leader of the Knights of Stromgald\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 700 of 822. Loss:0.7472415566444397. Time:0:10:39\n",
            "Example output: ION\"We live in harmony, with our dead loved ones fed on love, and we are the foundation of this world.\"\n",
            "—Kylo the Elder\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 800 of 822. Loss:0.6744431853294373. Time:0:12:11\n",
            "Example output:  glimpse\"The past is no excuse to fall prey to the ravings of the present. If we fail to heed their warnings, what is our duty?\"\n",
            "—Rukarumel, elder druid of the Juniper Order\n",
            "Average Training Loss: 0.8532402965346683. Epoch time: 0:12:31\n",
            "Validation loss: 0.6574541187804678. Validation Time: 0:00:29\n",
            "Beginning epoch 3 of 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 100 of 822. Loss:0.6345912218093872. Time:0:01:31\n",
            "Example output:  LaureWhen the thunderous low notes echo through the trees, all things benefit.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 200 of 822. Loss:0.6756399273872375. Time:0:03:03\n",
            "Example output: ismA giant of a man, the giant of a woman. A monster of a woman.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 300 of 822. Loss:0.6838239431381226. Time:0:04:34\n",
            "Example output: oun\"The next of the dead. The living... / But death has no name.\"\n",
            "—Ertai, wizard adept\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 400 of 822. Loss:0.6175540089607239. Time:0:06:05\n",
            "Example output:  electionOnly those gifted with the Immortal Sun will see their Immortal Sun.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 500 of 822. Loss:0.5614683032035828. Time:0:07:36\n",
            "Example output:  crazy\"The world grows rot with rot. Let's just hope that we don't rot the whole world.\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 600 of 822. Loss:0.531876266002655. Time:0:09:08\n",
            "Example output:  bench\"The sea is in my blood and I am yours. Behold the beauty of the world, the way in which you lived before me.\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 700 of 822. Loss:0.49778780341148376. Time:0:10:39\n",
            "Example output:  incorporatedKongming's troops never had time to waste on cavalry.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 800 of 822. Loss:0.544672429561615. Time:0:12:10\n",
            "Example output: Peter\"My heart was once a hundred li—\"\n",
            "Average Training Loss: 0.558850337409045. Epoch time: 0:12:30\n",
            "Validation loss: 0.5811365421699441. Validation Time: 0:00:29\n",
            "Beginning epoch 4 of 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 100 of 822. Loss:0.4965227544307709. Time:0:01:31\n",
            "Example output: uringThe only question the mages ask is, \"Will my sword blow away your skull?\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 200 of 822. Loss:0.46999651193618774. Time:0:03:02\n",
            "Example output:  reproductiveWhen provoked, it launches outward to find a mate.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 300 of 822. Loss:0.4927615821361542. Time:0:04:33\n",
            "Example output:  zone\"Where the flesh grows strongest, the bones will do.\"\n",
            "—Vorinclex, Voice of Hunger\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 400 of 822. Loss:0.4562358260154724. Time:0:06:04\n",
            "Example output:  commits\"The people of Esper measure their wealth by the number of servants they command.\"\n",
            "—Lazav\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 500 of 822. Loss:0.448271244764328. Time:0:07:35\n",
            "Example output:  ironyWhen the land is angry, so are they.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 600 of 822. Loss:0.43073570728302. Time:0:09:07\n",
            "Example output:  Sah\"How could it have imagined they could trade in their dreams for something else?\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 700 of 822. Loss:0.47097498178482056. Time:0:10:38\n",
            "Example output:  BryanIt's true most trees are ugly. They live in total darkness, so they make a perfect cover for the human eyes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 800 of 822. Loss:0.47759246826171875. Time:0:12:09\n",
            "Example output:  spirits\"Your bones will rot and rust, your flesh will rot, your mind will blacken, and your heart will shrivel.\"\n",
            "—Dravash, Gruul shaman\n",
            "Average Training Loss: 0.44866578806635815. Epoch time: 0:12:29\n",
            "Validation loss: 0.5606984243444775. Validation Time: 0:00:29\n",
            "Total training took 0:52:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQTvJ1vRP7u4"
      },
      "source": [
        "Let's view the summary of the training process, We'll visualize the change in training and validation loss to see if the models is converging. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68xreA9JAmG5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "60690c73-b17d-4874-a1b6-5beca7d7fe59"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "pd.set_option('precision', 2)\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2079Qyn8Mt8"
      },
      "source": [
        "# Saving & Loading Fine-Tuned Model\n",
        "\n",
        "Next it is a good idea to move the model we have trained and the associated weights, biases and model parameters out of the Colab space and into our own google drives. The cell below will mount your drive for you, then you can save the model using the prebuilt Hugging Face and PyTorch functionalities.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiUCVSGwSMgM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ulTWaOr8QNY"
      },
      "source": [
        "import os\n",
        "\n",
        "output_dir = '/content/drive/My Drive/MTG'\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLf6rbRglYhQ"
      },
      "source": [
        "# Generate Text\n",
        "\n",
        "Lastly the fun part! We will now generate samples from our model to test how well our model performs!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4XhewaV93-_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "f14c31f3-afb0-4f03-fa2c-fff673d4ec8c"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"<|startoftext|>\"\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "                                generated, \n",
        "                                do_sample=True,   \n",
        "                                top_k=50, \n",
        "                                max_length = 300,\n",
        "                                top_p=0.95, \n",
        "                                num_return_sequences=3\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: He raged at the world and at his family. But mostly he just raged.\n",
            "\n",
            "\n",
            "1: The more you struggle against it, the harder it adopts its hunger.\n",
            "\n",
            "\n",
            "2: \"There are relatively few tales of pirates. There's only one myth of a pirate landing without a weapon.\"\n",
            "—Kadri, Captain of the Guard\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBKFp7XBKZ8w"
      },
      "source": [
        "If you are familiar with MTG then you can evaluate these for yourself to determine how well the model has worked, or if you have modified the code to accept your own data input for a specific task this is what you are looking to evaluate for your own uses. Lastly, if you want to load the model you have saved to your google drive, the next cell will load the fine tuned GPT2 model and tokenizer, this means you can also share your model with other!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHJXRjDdvbmO"
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}